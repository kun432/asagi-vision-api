import base64
import binascii  # For base64.binascii.Error
import io
import logging
import os
import re
from collections.abc import AsyncIterator
from contextlib import asynccontextmanager

import requests
import torch
import uvicorn
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from PIL import Image
from pydantic import BaseModel, Field  # Moved to top with other third-party imports
from transformers import AutoModel, AutoProcessor, BitsAndBytesConfig

# Load environment variables from .env file
load_dotenv()

# Configuration
HOST = os.getenv("HOST", "0.0.0.0")
PORT = int(os.getenv("PORT", 8000))
LOG_LEVEL = os.getenv("LOG_LEVEL", "info").lower()
ROOT_PATH = os.getenv("ROOT_PATH", "")
QUANTIZATION_MODE = os.getenv("QUANTIZATION_MODE", "none").lower()
MODEL_NAME = os.getenv("MODEL_NAME", "MIL-UT/Asagi-2B")

# Logging setup
logging.basicConfig(level=LOG_LEVEL.upper(), format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# --- Global variables for Model and Processor ---
model = None
processor = None
device = "cuda" if torch.cuda.is_available() else "cpu"


@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncIterator[None]:
    # Load the ML model
    logger.info("Application startup via lifespan...")
    global model, processor
    model_path = MODEL_NAME
    logger.info(f"Loading Asagi-Vision model and processor from {model_path}...") # device is determined by device_map="auto"
    logger.info(f"Quantization mode: {QUANTIZATION_MODE}")

    quantization_config_params = {}
    if QUANTIZATION_MODE == "4bit":
        quantization_config_params["load_in_4bit"] = True
    elif QUANTIZATION_MODE == "8bit":
        quantization_config_params["load_in_8bit"] = True

    quantization_config = None
    if quantization_config_params:
        quantization_config = BitsAndBytesConfig(**quantization_config_params)

    try:
        processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
        model_kwargs = {
            "device_map": "auto",  # Let transformers handle device placement
            "torch_dtype": torch.bfloat16,
            "trust_remote_code": True,
        }
        if quantization_config:
            model_kwargs["quantization_config"] = quantization_config

        model = AutoModel.from_pretrained(model_path, **model_kwargs)
        # device will be set by device_map="auto", update global device if needed for other parts
        global device
        device = model.device # Update global device based on where the model is loaded
        logger.info(f"Asagi-Vision model and processor loaded successfully on {device}.")
    except Exception as e:
        logger.error(f"Error loading Asagi-Vision model/processor: {e}", exc_info=True)
        # Optionally re-raise or handle as critical startup failure
    yield
    # Clean up the ML models and release the resources
    logger.info("Application shutdown via lifespan...")
    # Add any cleanup logic here if needed in the future


app = FastAPI(title="Asagi-Vision API", version="0.1.0", root_path=ROOT_PATH, lifespan=lifespan)

# --- Pydantic Models for Asagi API ---


class AsagiGenerateRequest(BaseModel):
    prompt: str = Field(..., description="User's text prompt for the model.")
    image: str | None = Field(default=None, description="URL or Base64 encoded image data.")
    max_new_tokens: int | None = Field(default=128, ge=1, description="Maximum number of new tokens to generate.")
    temperature: float | None = Field(default=0.0, ge=0.0, le=2.0, description="Sampling temperature.")


class AsagiGenerateResponse(BaseModel):
    generated_text: str = Field(..., description="The text generated by the model.")


# Asagi-specific endpoints will be added here
@app.post(
    "/generate",
    response_model=AsagiGenerateResponse,
    summary="Generate text based on prompt and optional image.",
)
async def generate_handler(request_data: AsagiGenerateRequest) -> AsagiGenerateResponse:
    logger.info(f"Received /generate request: {request_data.model_dump_json(indent=2)}")

    if model is None or processor is None:
        logger.error("Model or processor not loaded. Cannot process request.")
        raise HTTPException(status_code=503, detail="Model is not ready, please try again later.")

    try:
        image_pil = None
        if request_data.image:
            image_input = request_data.image
            if image_input.startswith("http://") or image_input.startswith("https://"):
                logger.info(f"Fetching image from URL: {image_input}")
                try:
                    response = requests.get(image_input, stream=True, timeout=10)
                    response.raise_for_status()
                    image_pil = Image.open(io.BytesIO(response.content)).convert("RGB")
                    logger.info("Image fetched and converted to RGB successfully.")
                except requests.exceptions.RequestException as e:
                    logger.error(f"Failed to fetch image from URL '{image_input}': {e}")
                    raise HTTPException(status_code=400, detail=f"Failed to fetch image from URL: {str(e)}") from e
                except Exception as e:
                    logger.error(f"Invalid image at URL '{image_input}': {e}")
                    raise HTTPException(status_code=400, detail=f"Invalid image at URL: {str(e)}") from e
            else:
                logger.info("Processing Base64 encoded image.")
                try:
                    base64_data_str = image_input
                    if "," in base64_data_str:  # Handle data URI prefix
                        base64_data_str = base64_data_str.split(",", 1)[1]

                    if not re.fullmatch(r"[A-Za-z0-9+/]*={0,2}", base64_data_str):
                        logger.warning("Base64 string contains invalid characters.")
                        raise HTTPException(status_code=400, detail="Base64 string contains invalid characters.")

                    image_bytes = base64.b64decode(base64_data_str.encode("ascii"))
                    image_pil = Image.open(io.BytesIO(image_bytes)).convert("RGB")
                    # images_for_processor is no longer used, image_pil is used directly
                    logger.info("Base64 image decoded and converted to RGB successfully.")
                except binascii.Error as e:
                    logger.error(f"Invalid base64 encoding: {e}")
                    raise HTTPException(status_code=400, detail=f"Invalid base64 encoding: {str(e)}") from e
                except HTTPException:  # Re-raise if it's an HTTPException we threw
                    raise
                except Exception as e:
                    logger.error(f"Error processing base64 image: {e}", exc_info=True)
                    raise HTTPException(status_code=500, detail=f"Error processing base64 image: {str(e)}") from e

        # Prepare inputs for the Asagi model
        # Define effective_prompt based on Asagi's expected format if an image is present
        if image_pil:
            effective_prompt = (
                f"以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。\n\n"
                f"### 指示:\n<image>\n{request_data.prompt}\n\n### 応答:\n"
            )
        else:
            effective_prompt = request_data.prompt

        logger.debug(
            f"Preparing inputs for processor. Effective Prompt: {effective_prompt!r}, "
            f"Image: {'Present' if image_pil else 'Not present'}"
        )

        if image_pil:
            inputs_dict = processor(
                text=effective_prompt, images=image_pil, return_tensors="pt", padding=True, truncation=True
            )
        else:
            inputs_dict = processor(
                text=effective_prompt, images=None, return_tensors="pt", padding=True, truncation=True
            )

        # According to Asagi documentation, explicitly set input_ids and attention_mask from tokenizer for text part
        # This is crucial when images are present, ensuring <image> token is correctly processed.
        # If no image, this effectively re-tokenizes the same prompt, which is redundant but harmless.
        # For consistency and to ensure the tokenizer used by `processor` for text matches what `model.generate` expects for `input_ids`,
        # we re-tokenize the `effective_prompt` to get `input_ids` and `attention_mask`.
        inputs_text_tokenized = processor.tokenizer(effective_prompt, return_tensors="pt", padding=True, truncation=True)
        inputs_dict['input_ids'] = inputs_text_tokenized['input_ids']
        inputs_dict['attention_mask'] = inputs_text_tokenized['attention_mask']

        final_inputs = {}
        for k, v_tensor in inputs_dict.items():
            if k == "token_type_ids":
                continue  # Skip token_type_ids as it's not used by some models like Asagi
            if torch.is_tensor(v_tensor):
                if v_tensor.dtype == torch.float32 and model.dtype == torch.bfloat16:
                    final_inputs[k] = v_tensor.to(model.dtype).to(model.device)
                # input_ids (torch.long) and others should only be moved to model.device
                else:
                    final_inputs[k] = v_tensor.to(model.device)
            else:
                final_inputs[k] = v_tensor # Should not happen for standard processor output
        inputs = final_inputs
        logger.debug("Inputs prepared (with tokenizer override for text if image present) and moved to device.")

        temp_for_generation = request_data.temperature
        if temp_for_generation is None:
            # Pydantic default is 0.0 if field omitted; if null, it's None here.
            # Model.generate usually defaults temperature to 1.0 if not provided or if None implies default.
            # Let's use 0.0 to be consistent with Pydantic's omission default.
            temp_for_generation = 0.0

        generate_kwargs = {
            "max_new_tokens": request_data.max_new_tokens,
            "temperature": temp_for_generation,
            "do_sample": temp_for_generation > 0.0,
        }
        if hasattr(processor, "tokenizer") and processor.tokenizer is not None:
            if processor.tokenizer.pad_token_id is not None:
                generate_kwargs["pad_token_id"] = processor.tokenizer.pad_token_id
            if processor.tokenizer.eos_token_id is not None:
                generate_kwargs["eos_token_id"] = processor.tokenizer.eos_token_id

        # Stopping criteria for Sarashina2 is removed as Asagi might not use/need it in the same way.
        # If specific stopping criteria are needed for Asagi, they should be added here based on its documentation.
        logger.info(f"Starting model generation with args: {generate_kwargs}")

        with torch.no_grad():
            output_ids = model.generate(**inputs, **generate_kwargs)
        logger.info("Model generation completed.")

        # Decode the generated IDs. Asagi example uses clean_up_tokenization_spaces=False.
        # The output_ids from model.generate include the input prompt tokens.
        generated_text_with_prompt = processor.batch_decode(
            output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )[0]  # Assuming batch size is 1 for API requests

        # Remove the input prompt from the generated text
        # (as per asagi-2b.md example, though it handles <image> tag differently)
        # Use the same effective_prompt that was fed to the model for removal
        prompt_text_for_removal = effective_prompt
        # Asagi's example also removes <image> placeholder from prompt before stripping, if present
        if "<image>" in prompt_text_for_removal:
            prompt_text_for_removal_cleaned = prompt_text_for_removal.replace("<image>", " ") # Replace with space as per example
        else:
            prompt_text_for_removal_cleaned = prompt_text_for_removal

        if generated_text_with_prompt.startswith(prompt_text_for_removal_cleaned):
            output_text = generated_text_with_prompt[len(prompt_text_for_removal_cleaned):]
        elif generated_text_with_prompt.startswith(prompt_text_for_removal): # Fallback for safety, if cleaned version didn't match but original did
            output_text = generated_text_with_prompt[len(prompt_text_for_removal):]
        else:
            # If the generated text doesn't start with the prompt (neither cleaned nor original), log a warning and use the full text.
            # This might happen if the model's output format is unexpected.
            logger.warning(
                f"Generated text does not start with the input prompt. "
                f"Cleaned Prompt for removal: {prompt_text_for_removal_cleaned!r}, Original Prompt: {prompt_text_for_removal!r}, Generated: {generated_text_with_prompt!r}"
            )
            output_text = generated_text_with_prompt

        logger.info(f"Generated text: {output_text!r}")
        return AsagiGenerateResponse(generated_text=output_text)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error during generation: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"An internal error occurred: {str(e)}") from e


if __name__ == "__main__":
    logger.info(f"Starting Asagi-Vision API server on http://{HOST}:{PORT}{ROOT_PATH}")
    uvicorn.run("server:app", host=HOST, port=PORT, log_level=LOG_LEVEL, reload=True)
